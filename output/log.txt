Retained 2000 words from 9954 (88.81% of all tokens)

Parameter tuning of hidden_dims, lookback, lr: 
[(25, 0, 0.5), (25, 0, 0.1), (25, 0, 0.05), (25, 0, 0.01), (25, 2, 0.5), (25, 2, 0.1), (25, 2, 0.05), (25, 2, 0.01), (25, 5, 0.5), (25, 5, 0.1), (25, 5, 0.05), (25, 5, 0.01), (50, 0, 0.5), (50, 0, 0.1), (50, 0, 0.05), (50, 0, 0.01), (50, 2, 0.5), (50, 2, 0.1), (50, 2, 0.05), (50, 2, 0.01), (50, 5, 0.5), (50, 5, 0.1), (50, 5, 0.05), (50, 5, 0.01), (100, 0, 0.5), (100, 0, 0.1), (100, 0, 0.05), (100, 0, 0.01), (100, 2, 0.5), (100, 2, 0.1), (100, 2, 0.05), (100, 2, 0.01), (100, 5, 0.5), (100, 5, 0.1), (100, 5, 0.05), (100, 5, 0.01)]
Total experiments 36
Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.5, annealing set to 5
calculating initial mean loss on dev set
: 7.798662515757492

epoch 1, learning rate 0.5000
